{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naman-Priyadarshi/WebSem_Project/blob/main/WebSem_Project_Creating_a_Knowledge_Graph_for_Cycling_Domain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WebSem Project: Constructing and Querying a Knowledge Graph in the Cycling Domain\n",
        "\n",
        "## Introduction\n",
        "\n",
        "The goal of this project is to extract information from multilingual textual documents about cycling and create a knowledge graph (KG) using the extracted entities and relations. The KG will be compatible with a cycling ontology and queries will be written in SPARQL to retrieve specific information from the KG. The project will be implemented using Jupyter Notebook and the following steps will be followed:\n",
        "\n",
        "* Collect multilingual textual documents about cycling.\n",
        "* Pre-process the documents to get clean text files.\n",
        "* Run named entity recognition (NER) on the documents to extract named entities of the type Person, Organization and Location using spaCy and LLMs.\n",
        "* Run co-reference resolution on the input text using spaCy.\n",
        "* Disambiguate the entities with Wikidata using OpenTapioca and LLMs.\n",
        "* Run relation extraction using Stanford OpenIE and LLMs.\n",
        "* Implement some mappings between the entity types and relations returned with the cycling ontology you developed during the Assignment 1 in order to create a knowledge graph of the domain represented in RDF.\n",
        "* Load the data in the Corese engine as you did for the Assignment 2 with your cycling ontology and the knowledge graph built in the previous step and write some SPARQL queries to retrieve specific information from the KG.\n",
        "\n",
        "### Useful resources\n",
        "* The github repository \"Building knowledge graph from input data\" at  https://github.com/varun196/knowledge_graph_from_unstructured_text can be used as an inspiration.\n",
        "\n",
        "### References\n",
        "* NLTK: https://www.nltk.org/\n",
        "* spaCy: https://spacy.io/\n",
        "* Stanford OpenIE: https://nlp.stanford.edu/software/openie.html\n",
        "* OpenTapioca: https://opentapioca.org/\n",
        "* Corese engine: https://project.inria.fr/corese/\n",
        "* Wikidata: https://www.wikidata.org/"
      ],
      "metadata": {
        "id": "mpecDDB_GGLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Collect multilingual textual documents about cycling\n",
        "For this mini project, we will collect multilingual textual documents about cycling from various sources such as news articles, blog posts, and race reports. We will download the documents and save them in a directory called `cycling_docs`.\n",
        "\n",
        "The list of documents to download are available at:\n",
        "\n",
        "* English:\n",
        " - https://en.wikipedia.org/wiki/2022_Tour_de_France\n",
        " - https://en.wikipedia.org/wiki/2022_Tour_de_France,_Stage_1_to_Stage_11\n",
        " - https://en.wikipedia.org/wiki/2022_Tour_de_France,_Stage_12_to_Stage_21\n",
        " - https://www.bbc.com/sport/cycling/61940037\n",
        " - https://www.bbc.com/sport/cycling/62017114 (stage 1)\n",
        " - https://www.bbc.com/sport/cycling/62097721 (stage 7)\n",
        " - https://www.bbc.com/sport/cycling/62153759 (stage 11)\n",
        " - https://www.bbc.co.uk/sport/cycling/62285420 (stage 21)\n",
        "\n",
        "* French:\n",
        " - https://fr.wikipedia.org/wiki/Tour_de_France_2022\n",
        " - https://www.francetvinfo.fr/tour-de-france/tour-de-france-2022-epoustouflant-jonas-vingegaard-remporte-la-11e-etape-et-s-empare-du-maillot-jaune-de-tadej-pogacar_5254102.html\n",
        " - https://www.francetvinfo.fr/tour-de-france/tour-de-france-2022-jonas-vingegaard-vainqueur-de-sa-premiere-grande-boucle-jasper-philipsen-s-offre-au-sprint-la-21e-etape_5275612.html"
      ],
      "metadata": {
        "id": "EzMinLFxGUFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Feel free to install more dependencies if needed!\n",
        "#\n",
        "\n",
        "# Install requests for querying HTTP endpoints\n",
        "!pip install --quiet requests\n",
        "\n",
        "# Install jusText for automatically extracting text from web pages\n",
        "!pip install --quiet jusText\n",
        "\n",
        "# Install nltk for text processing\n",
        "!pip install --quiet nltk\n",
        "\n",
        "# Install spaCy for NER extraction\n",
        "!pip install --quiet spacy\n",
        "\n",
        "# Install pycorenlp for Stanford CoreNLP\n",
        "!pip install --quiet pycorenlp\n",
        "\n",
        "# Install pandas for data visualization\n",
        "!pip install --quiet pandas\n",
        "\n",
        "# Install rdflib for writing RDF\n",
        "!pip install --quiet rdflib"
      ],
      "metadata": {
        "id": "HccrPk8uGVz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "import requests\n",
        "import justext\n",
        "import os\n",
        "from urllib.parse import urlsplit\n",
        "\n",
        "\n",
        "# Define a function to get filename from URL\n",
        "def get_filename_from_url(url):\n",
        "  urlpath = urlsplit(url).path\n",
        "  return os.path.basename(urlpath)\n",
        "\n",
        "\n",
        "# Define a function to download URLs and extract text\n",
        "def download_urls(urls_list, language):\n",
        "  # Loop over each URL in the list\n",
        "  for url in urls_list:\n",
        "    # Fetch and extract text from the URL using jusText\n",
        "    response = requests.get(url)\n",
        "    paragraphs = justext.justext(\n",
        "      response.content,\n",
        "      justext.get_stoplist(language.capitalize()),\n",
        "      no_headings=True,\n",
        "      max_heading_distance=150,\n",
        "      length_low=70,\n",
        "      length_high=140,\n",
        "      stopwords_low=0.2,\n",
        "      stopwords_high=0.3,\n",
        "      max_link_density=0.4\n",
        "    )\n",
        "    extracted_text = '\\n'.join(list(filter(None, map(\n",
        "      lambda paragraph: paragraph.text if not paragraph.is_boilerplate else '',\n",
        "      paragraphs\n",
        "    ))))\n",
        "\n",
        "    # Truncate text if it's too long\n",
        "    extracted_text = extracted_text[0:10000]\n",
        "\n",
        "    # Create the output directory if it does not exist\n",
        "    output_dir = os.path.join('cycling_docs', language)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Save extracted text as a .txt file\n",
        "    filename = get_filename_from_url(url)\n",
        "    output_path = os.path.join(output_dir, f'{filename}.txt')\n",
        "    with open(output_path, 'w') as f:\n",
        "      f.write(extracted_text)\n",
        "\n",
        "    print(f'Downloaded {url} into {output_path}')\n",
        "\n",
        "\n",
        "# List of URLs to download\n",
        "urls_list_english = [\n",
        "  'https://en.wikipedia.org/wiki/2022_Tour_de_France',\n",
        "  'https://en.wikipedia.org/wiki/2022_Tour_de_France,_Stage_1_to_Stage_11',\n",
        "  'https://en.wikipedia.org/wiki/2022_Tour_de_France,_Stage_12_to_Stage_21',\n",
        "  'https://www.bbc.com/sport/cycling/61940037',\n",
        "  'https://www.bbc.com/sport/cycling/62017114',\n",
        "  'https://www.bbc.com/sport/cycling/62097721',\n",
        "  'https://www.bbc.com/sport/cycling/62153759',\n",
        "  'https://www.bbc.co.uk/sport/cycling/62285420',\n",
        "]\n",
        "urls_list_french = [\n",
        "  'https://fr.wikipedia.org/wiki/Tour_de_France_2022',\n",
        "  'https://www.francetvinfo.fr/tour-de-france/tour-de-france-2022-epoustouflant-jonas-vingegaard-remporte-la-11e-etape-et-s-empare-du-maillot-jaune-de-tadej-pogacar_5254102.html',\n",
        "  'https://www.francetvinfo.fr/tour-de-france/tour-de-france-2022-jonas-vingegaard-vainqueur-de-sa-premiere-grande-boucle-jasper-philipsen-s-offre-au-sprint-la-21e-etape_5275612.html',\n",
        "]\n",
        "\n",
        "# Download the listed URLs\n",
        "download_urls(urls_list_english, 'english')\n",
        "download_urls(urls_list_french, 'french')"
      ],
      "metadata": {
        "id": "_031XjozIHqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Pre-process the documents to get clean txt files\n",
        "We will pre-process the documents to get clean txt files by removing any unnecessary characters, punctuation, and stopwords. We will use Python's [re](https://docs.python.org/3/library/re.html) and [nltk](https://www.nltk.org/) libraries for this purpose. We will save the results in a `clean_docs` folder."
      ],
      "metadata": {
        "id": "Wg52A5ELGWvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Document class which holds all the necessary variables for the purpose of this\n",
        "project.\n",
        "\"\"\"\n",
        "class Document:\n",
        "  def __init__(self, text, language = None, raw_text = None, filepath = None):\n",
        "    self.filepath = filepath    # Path to the document file\n",
        "    self.language = language    # Language of the document\n",
        "    self.raw_text = raw_text    # Origial text before cleaning\n",
        "    self.cleaned_text = text    # Text after cleaning (Step 2)\n",
        "    self.spacy_entities = []    # List of spaCy entities (Step 3a)\n",
        "    self.llm_entities = []      # List of LLM entities (Step 3b)\n",
        "    self.resolved_text = None   # Text after resolving co-references (Step 4)\n",
        "    self.coreferences = None    # CoreNLP coreferences object (Step 4)\n",
        "    self.wiki_entities = {}     # Dictionary of Wikidata entities extracted with OpenTapioca (Step 5a)\n",
        "    self.llm_wiki_entities = {} # Dictionary of Wikidata entities extracted with LLMs (Step 5b)\n",
        "    self.relations = []         # List of OpenIE relations (Step 6a)\n",
        "    self.llm_relations = []     # List of LLM relations (Step 6b)"
      ],
      "metadata": {
        "id": "t9sdmals1W7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üìù TODO: Import the necessary libraries for natural language processing\n",
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def clean_text(dirty_text, language):\n",
        "  # üìù TODO: Define a function to clean text (words tokenization, stopwords\n",
        "  #          removal, ...).\n",
        "  # Tokenize the text into words\n",
        "  tokens = word_tokenize(dirty_text)\n",
        "\n",
        "  # Convert tokens to lowercase and remove punctuation\n",
        "  tokens = [word.lower() for word in tokens if word.isalnum()]\n",
        "\n",
        "  # Remove stopwords\n",
        "  stop_words = set(stopwords.words(language))\n",
        "  cleaned_text = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "  # Return the cleaned text\n",
        "  return cleaned_text"
      ],
      "metadata": {
        "id": "k9JsLAGsxsyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to process a file and write the result to a new file\n",
        "def process_file(file, language):\n",
        "  # Open the file in read-only mode and read all of its lines\n",
        "  with open(file, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "  # Concatenate all the lines into a single string\n",
        "  raw_text = '\\n'.join(lines)\n",
        "\n",
        "  # Clean the text using the `clean_text` function\n",
        "  cleaned_text = clean_text(raw_text, language)\n",
        "\n",
        "  # Create a new document and return it\n",
        "  doc = Document(cleaned_text, language=language, raw_text=raw_text, filepath=os.path.abspath(file))\n",
        "  return doc\n",
        "\n",
        "\n",
        "# Create a list to store all our documents\n",
        "docs = []\n",
        "\n",
        "# Loop through all the files in the \"cycling_docs\" folder\n",
        "folder = 'cycling_docs'\n",
        "for language in os.listdir(folder):\n",
        "  for filename in os.listdir(os.path.join(folder, language)):\n",
        "    # Construct the full path to the file\n",
        "    file = os.path.join(folder, language, filename)\n",
        "\n",
        "    # Check if the file is a regular file and has a .txt extension\n",
        "    if os.path.isfile(file) and file.endswith('.txt'):\n",
        "      # Process the file and append the new Document to our list\n",
        "      doc = process_file(file, language)\n",
        "      docs.append(doc)"
      ],
      "metadata": {
        "id": "KhFqE5TleLz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the text of the first document\n",
        "display(docs[0].cleaned_text)"
      ],
      "metadata": {
        "id": "yi3wcREO1plh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Run named entity recognition (NER) on the documents\n",
        "\n",
        "The goal of this step is to extract named entities from the text of our documents. We will attempt to use two methods: spaCy, and LLMs."
      ],
      "metadata": {
        "id": "nXH__c_rGaIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3a: Using spaCy\n",
        "\n",
        "We will use [spaCy](https://spacy.io)'s pre-trained models to perform NER on the documents and extract the entities of type PER/ORG/LOC. The extracted entities will be saved in a file.\n",
        "\n",
        "**‚ö†Ô∏è Important Note:** We must use the raw text files (before the cleaning step in Step 2) for NER to ensure we do not lose context or critical information needed for accurate entity recognition."
      ],
      "metadata": {
        "id": "XFsIQKlQoqfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìù TODO: Import spaCy and other libraries that might be required for entity\n",
        "#          extraction\n",
        "\n",
        "import spacy\n",
        "\n",
        "def extract_entities(text, language):\n",
        "  # üìù TODO: Use spaCy to extract named entities and store them into a list.\n",
        "  # The format of the end result should look like this:\n",
        "  # ```\n",
        "  # entities = [\n",
        "  #   { \"text\": \"Tour de France\", \"label\": \"ORG\" },\n",
        "  #   { \"text\": \"Peter Sagan\", \"label\": \"PERSON\" },\n",
        "  # ]\n",
        "  # ```\n",
        "\n",
        "# Load the spaCy language model (already installed with `en_core_web_sm`)\n",
        "  # Map language to appropriate SpaCy model\n",
        "  if language.lower() == \"english\":\n",
        "      model = \"en_core_web_sm\"\n",
        "  elif language.lower() == \"french\":\n",
        "      model = \"fr_core_news_sm\"\n",
        "  else:\n",
        "      raise ValueError(f\"Unsupported language: {language}\")\n",
        "\n",
        "  # Load the SpaCy model\n",
        "  nlp = spacy.load(model)\n",
        "\n",
        "  # Process the text with spaCy\n",
        "  doc = nlp(text)\n",
        "\n",
        "  # Extract named entities\n",
        "  entities = []\n",
        "  for ent in doc.ents:\n",
        "    entities.append({\"text\": ent.text, \"label\": ent.label_})\n",
        "\n",
        "  return entities"
      ],
      "metadata": {
        "id": "MkMYXqVFGy9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_sm"
      ],
      "metadata": {
        "id": "wTTw62CDr4mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract entities for each document\n",
        "for doc in docs:\n",
        "  doc.spacy_entities = extract_entities(doc.raw_text, doc.language)"
      ],
      "metadata": {
        "id": "REq50-k7ePSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display entities which have been extracted:"
      ],
      "metadata": {
        "id": "NkFvU6OfDwux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìù TODO: Display the extracted entities for the first document\n",
        "# Check if there are documents in the list\n",
        "if docs:\n",
        "    # Get the first document\n",
        "    first_doc = docs[0]\n",
        "\n",
        "    # Print the file path and language of the document\n",
        "    print(f\"Document Path: {first_doc.filepath}\")\n",
        "    print(f\"Language: {first_doc.language}\")\n",
        "\n",
        "    # Print the extracted entities\n",
        "    print(\"\\nExtracted Entities:\")\n",
        "    for entity in first_doc.spacy_entities:\n",
        "        print(f\"Text: {entity['text']}, Label: {entity['label']}\")\n",
        "else:\n",
        "    print(\"No documents available to display.\")\n"
      ],
      "metadata": {
        "id": "ER2xsq4WYJgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3b: Using LLMs\n",
        "\n",
        "**‚ö†Ô∏è Important Note:** We must use the raw text files (before the cleaning step in Step 2) for NER to ensure we do not lose context or critical information needed for accurate entity recognition.\n",
        "\n",
        "First we create a function `llm_generate` which calls an [Ollama](https://ollama.com/) server hosted at EURECOM. For the purpose of this exercise, you are limited to using a specific model (`mistral-nemo:12b-instruct-2407-fp16`). The full documentation for the API is available at: https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-completion"
      ],
      "metadata": {
        "id": "W-7kzYQXsB4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "# Define a function to call the Ollama WebSem endpoint using a given payload and return the response.\n",
        "def llm_generate(payload):\n",
        "  # Define the API endpoint and payload\n",
        "  url = \"https://websem:eurecom@ollama-websem.tools.eurecom.fr/api/generate\"\n",
        "  payload[\"model\"] = \"mistral-nemo:12b-instruct-2407-fp16\"\n",
        "  payload[\"stream\"] = \"false\"\n",
        "\n",
        "  # Define the headers\n",
        "  headers = {\n",
        "    \"Content-Type\": \"application/json\"\n",
        "  }\n",
        "\n",
        "  # Send the POST request\n",
        "  response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
        "\n",
        "  # Check if the request was successful\n",
        "  if response.status_code == 200:\n",
        "    # Parse the JSON response\n",
        "    response_data = response.json()\n",
        "\n",
        "    # Return the structured entities\n",
        "    return response_data\n",
        "  else:\n",
        "    # Handle errors\n",
        "    print(f\"Error {response.status_code}: {response.text}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "KqUP7q2dnkfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's learn how to use it:"
      ],
      "metadata": {
        "id": "HL8AiQp8atUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example with basic prompt\n",
        "payload = {\n",
        "  \"prompt\": \"\"\"\n",
        "  Compose a haiku about web semantics, emphasizing the interconnectedness of\n",
        "  data, the elegance of structured knowledge, and the power of understanding\n",
        "  through linked information.\n",
        "  \"\"\"\n",
        "}\n",
        "llm_response = llm_generate(payload)\n",
        "print(llm_response[\"response\"])"
      ],
      "metadata": {
        "id": "FDaZj-VzYZPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can even use structured outputs. More informations are available at:\n",
        "* https://ollama.com/blog/structured-outputs\n",
        "* https://github.com/ollama/ollama/blob/main/docs/api.md#request-structured-outputs"
      ],
      "metadata": {
        "id": "NtOr2BtFaUGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example with structured outputs\n",
        "payload = {\n",
        "    \"prompt\": \"\"\"\n",
        "    Solve the following math problem and explain the steps clearly:\n",
        "\n",
        "    Problem:\n",
        "    What is 2 + 2?\n",
        "\n",
        "    Provide the solution and explanation in the following structure:\n",
        "    \"\"\",\n",
        "    \"format\": {\n",
        "        \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"solution\": {\"type\": \"integer\"},\n",
        "            \"explanation\": {\"type\": \"string\"}\n",
        "        },\n",
        "        \"required\": [\"solution\", \"explanation\"]\n",
        "    }\n",
        "}\n",
        "llm_response = llm_generate(payload)\n",
        "print(llm_response[\"response\"])"
      ],
      "metadata": {
        "id": "vJOMGlrsZZdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's create a function which can extract entities using `llm_generate` with a custom prompt. There are many ways to achieve this. For example, you could try providing examples, and/or using a JSON schema."
      ],
      "metadata": {
        "id": "SEHoqXoEbb5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_entities_with_llm(text):\n",
        "    # Define the payload for the LLM request\n",
        "    payload = {\n",
        "        \"prompt\": f\"\"\"\n",
        "        Extract named entities from the following text and classify them into appropriate categories such as PERSON, ORG, EVENT, LOCATION, etc.:\n",
        "\n",
        "        Text:\n",
        "        {text}\n",
        "\n",
        "        Provide the result in the following JSON structure:\n",
        "        [\n",
        "          {{ \"text\": \"<entity_text>\", \"label\": \"<entity_label>\" }},\n",
        "          ...\n",
        "        ]\n",
        "        \"\"\",\n",
        "        \"format\": {\n",
        "            \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"text\": {\"type\": \"string\"},\n",
        "                    \"label\": {\"type\": \"string\"}\n",
        "                },\n",
        "                \"required\": [\"text\", \"label\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Call the LLM generate function\n",
        "    llm_response = llm_generate(payload)\n",
        "\n",
        "    # Check if the response is valid\n",
        "    if llm_response and \"response\" in llm_response:\n",
        "        try:\n",
        "            # Parse the JSON response\n",
        "            entities = json.loads(llm_response[\"response\"])\n",
        "            return entities\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Error decoding JSON response.\")\n",
        "            return []\n",
        "    else:\n",
        "        print(\"Error generating response from LLM.\")\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "45flIC88sPAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract entities for each document\n",
        "for doc in docs:\n",
        "  doc.llm_entities = extract_entities_with_llm(doc.raw_text)"
      ],
      "metadata": {
        "id": "8s5r5lXMsc6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display entities which have been extracted:"
      ],
      "metadata": {
        "id": "PRoGApD6tFWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display entities for the first document\n",
        "display(docs[0].llm_entities)"
      ],
      "metadata": {
        "id": "0-xmKN4dtFWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Run co-reference resolution on the input text\n",
        "We will use CoreNLP to perform [co-reference resolution](https://en.wikipedia.org/wiki/Coreference) on the input text and resolve coreferences.\n",
        "\n",
        "For this project, we will use a hosted version of CoreNLP at: https://corenlp.tools.eurecom.fr/ (username: `websem`, password: `eurecom`). Feel free to try out the web interface before writing the code.\n",
        "\n",
        "First, we compute the annotations and store them into the `coreferences` variable of our Document:"
      ],
      "metadata": {
        "id": "lAsxblm4GceJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pycorenlp import StanfordCoreNLP\n",
        "\n",
        "\n",
        "# Set up the CoreNLP client\n",
        "nlp = StanfordCoreNLP('https://websem:eurecom@corenlp.tools.eurecom.fr')\n",
        "\n",
        "# Define a function which computes coreferences for a given text and language\n",
        "def compute_coreferences(text, language):\n",
        "  props = {\n",
        "    'timeout': 300000,\n",
        "    'annotators': 'tokenize,ssplit,coref',\n",
        "    'pipelineLanguage': language[:2],\n",
        "    'outputFormat': 'json'\n",
        "  }\n",
        "\n",
        "  # Annotate the text for co-reference resolution\n",
        "  corenlp_output = nlp.annotate(text, properties=props)\n",
        "  try:\n",
        "    corenlp_output = json.loads(corenlp_output)\n",
        "  except Exception as err:\n",
        "    print(f'Unexpected response: {corenlp_output}')\n",
        "    raise\n",
        "\n",
        "  return corenlp_output"
      ],
      "metadata": {
        "id": "v0WOTNW3Ggg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test co-references computation\n",
        "example = compute_coreferences(\"John is a software engineer. He is very talented. Sarah is a designer. She works with him.\", language=\"en\")\n",
        "\n",
        "# Pretty-print them\n",
        "print(json.dumps(example, indent=2))"
      ],
      "metadata": {
        "id": "ajQ2T6QyhWop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute co-references for all documents\n",
        "for doc in docs:\n",
        "  if doc.language == \"english\":  # CoreNLP Coref-resolution only supports english\n",
        "    doc.coreferences = compute_coreferences(doc.raw_text, doc.language)"
      ],
      "metadata": {
        "id": "FSnVwbZkhUii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step is to display all co-references for each mentions in the text.\n",
        "\n",
        "For example:\n",
        "\n",
        "> \"He\" -> \"John\"\n",
        ">\n",
        "> \"She\" -> \"Sarah\"\n",
        ">\n",
        "> \"him\" -> \"John\""
      ],
      "metadata": {
        "id": "iBqJ8DTtoFpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for coref_cluster in example['corefs'].values():\n",
        "  # üìù TODO: Print each co-references like so: \"He\" -> \"John\"\n",
        "  # üí° Each cluster has one representative mention, flagged with `isRepresentativeMention: True`\n",
        "  representative_mention = None\n",
        "  for mention in coref_cluster:\n",
        "      if mention['isRepresentativeMention']:\n",
        "          representative_mention = mention['text']\n",
        "          break\n",
        "\n",
        "  if representative_mention:\n",
        "      for mention in coref_cluster:\n",
        "          if not mention['isRepresentativeMention']:\n",
        "              print(f'\"{mention[\"text\"]}\" -> \"{representative_mention}\"')"
      ],
      "metadata": {
        "id": "vp2BHxDHoSUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üèÜ Challenge\n",
        "\n",
        "Replace values within the text with their resolved co-reference. For example, with the following text:\n",
        "\n",
        "> **John** is a software engineer. **He** is very talented.\n",
        "\n",
        "In the second sentence, the pronoun \"He\" would be replaced with its co-reference, and the final text would become:\n",
        "\n",
        "> **John** is a software engineer. **John** is very talented."
      ],
      "metadata": {
        "id": "bVhTqao-5Z55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function which resolves coreferences inside a document\n",
        "def resolve_coreferences(corenlp_output):\n",
        "\n",
        "  # üìù TODO: Replace values within the text with their resolved co-reference.\n",
        "  # üí° You can start by printing the `corenlp_output` object to understand its\n",
        "  #    structure.\n",
        "\n",
        "  #get the coreference clusters\n",
        "  coref_clusters = corenlp_output['corefs']\n",
        "\n",
        "  #initialize a list to store the resolved text pieces\n",
        "  resolved_text = []\n",
        "\n",
        "  #extract the sentences and their corresponding mentions\n",
        "  sentences = corenlp_output.get('sentences', [])\n",
        "\n",
        "  #flatten all mentions in the document and create a mapping of mentions to their representative\n",
        "  mention_to_representative = {}\n",
        "  for coref_cluster in coref_clusters.values():\n",
        "      representative_mention = None\n",
        "      for mention in coref_cluster:\n",
        "          if mention['isRepresentativeMention']:\n",
        "              representative_mention = mention['text']\n",
        "              break\n",
        "\n",
        "      #map all mentions in the cluster to the representative\n",
        "      for mention in coref_cluster:\n",
        "          mention_to_representative[mention['text']] = representative_mention\n",
        "\n",
        "  #replacing mentions with their representatives\n",
        "  for sentence in sentences:\n",
        "      sentence_text = ' '.join([token['originalText'] for token in sentence['tokens']])\n",
        "\n",
        "      for mention, representative in mention_to_representative.items():\n",
        "          sentence_text = sentence_text.replace(mention, representative)\n",
        "\n",
        "      sentence_text = sentence_text.strip()\n",
        "\n",
        "      #ensuring the sentence ends with a period, without extra spaces\n",
        "      if sentence_text and not sentence_text.endswith('.'):\n",
        "          sentence_text += '.'\n",
        "\n",
        "      resolved_text.append(sentence_text)\n",
        "\n",
        "  return ' '.join(resolved_text).replace(' .', '.')"
      ],
      "metadata": {
        "id": "V4P_mgqCeebD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test resolving co-references\n",
        "original_text = \"John is a software engineer. He is very talented. Sarah is a designer. She works with him.\"\n",
        "corefs = compute_coreferences(original_text, language=\"en\")\n",
        "resolved_text = resolve_coreferences(corefs)\n",
        "print(original_text)\n",
        "print(resolved_text)"
      ],
      "metadata": {
        "id": "5rXXcAtMklMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resolve co-references for all documents\n",
        "for doc in docs:\n",
        "  if doc.coreferences is not None:\n",
        "    doc.resolved_text = resolve_coreferences(doc.coreferences)"
      ],
      "metadata": {
        "id": "ntDsoAUikd4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üìù TODO: Display text with resolved co-references for the any document of your choice\n",
        "document_of_choice = docs[7]\n",
        "\n",
        "print(\"Resolved Text for the chosen document:\")\n",
        "print(document_of_choice.resolved_text)"
      ],
      "metadata": {
        "id": "hINiwG5V-__O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Disambiguate the entities"
      ],
      "metadata": {
        "id": "oIQZdbp9Gg6v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will reuse the same method `llm_generate` from Step 3, but with a different prompt in order to disambiguate the entities from Wikidata and DBpedia."
      ],
      "metadata": {
        "id": "VuIkf6vnrX-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def disambiguate_with_llm(text):\n",
        "    # Define the payload for the LLM request to disambiguate entities based on Wikidata\n",
        "    payload_wikidata = {\n",
        "        \"prompt\": f\"\"\"\n",
        "        Disambiguate the following named entities based on Wikidata. For each entity, provide its correct Wikidata entity ID and label:\n",
        "\n",
        "        Text:\n",
        "        {text}\n",
        "\n",
        "        Provide the result in the following JSON structure:\n",
        "        [\n",
        "          {{ \"text\": \"<entity_text>\", \"wikidata_id\": \"<wikidata_entity_id>\", \"label\": \"<wikidata_label>\" }},\n",
        "          ...\n",
        "        ]\n",
        "        \"\"\",\n",
        "        \"format\": {\n",
        "            \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"text\": {\"type\": \"string\"},\n",
        "                    \"wikidata_id\": {\"type\": \"string\"},\n",
        "                    \"label\": {\"type\": \"string\"}\n",
        "                },\n",
        "                \"required\": [\"text\", \"wikidata_id\", \"label\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Call the LLM generate function for Wikidata disambiguation\n",
        "    llm_response_wikidata = llm_generate(payload_wikidata)\n",
        "\n",
        "    # Define the payload for the LLM request to disambiguate entities based on DBpedia\n",
        "    payload_dbpedia = {\n",
        "        \"prompt\": f\"\"\"\n",
        "        Disambiguate the following named entities based on DBpedia. For each entity, provide its correct DBpedia resource URI and label:\n",
        "\n",
        "        Text:\n",
        "        {text}\n",
        "\n",
        "        Provide the result in the following JSON structure:\n",
        "        [\n",
        "          {{ \"text\": \"<entity_text>\", \"dbpedia_uri\": \"<dbpedia_resource_uri>\", \"label\": \"<dbpedia_label>\" }},\n",
        "          ...\n",
        "        ]\n",
        "        \"\"\",\n",
        "        \"format\": {\n",
        "            \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"text\": {\"type\": \"string\"},\n",
        "                    \"dbpedia_uri\": {\"type\": \"string\"},\n",
        "                    \"label\": {\"type\": \"string\"}\n",
        "                },\n",
        "                \"required\": [\"text\", \"dbpedia_uri\", \"label\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Call the LLM generate function for DBpedia disambiguation\n",
        "    llm_response_dbpedia = llm_generate(payload_dbpedia)\n",
        "\n",
        "    # Process the responses for Wikidata\n",
        "    if llm_response_wikidata and \"response\" in llm_response_wikidata:\n",
        "        try:\n",
        "            # Parse the JSON response for Wikidata\n",
        "            disambiguated_wikidata_entities = json.loads(llm_response_wikidata[\"response\"])\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Error decoding Wikidata JSON response.\")\n",
        "            disambiguated_wikidata_entities = []\n",
        "    else:\n",
        "        print(\"Error generating response from LLM for Wikidata.\")\n",
        "        disambiguated_wikidata_entities = []\n",
        "\n",
        "    # Process the responses for DBpedia\n",
        "    if llm_response_dbpedia and \"response\" in llm_response_dbpedia:\n",
        "        try:\n",
        "            # Parse the JSON response for DBpedia\n",
        "            disambiguated_dbpedia_entities = json.loads(llm_response_dbpedia[\"response\"])\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Error decoding DBpedia JSON response.\")\n",
        "            disambiguated_dbpedia_entities = []\n",
        "    else:\n",
        "        print(\"Error generating response from LLM for DBpedia.\")\n",
        "        disambiguated_dbpedia_entities = []\n",
        "\n",
        "    # Combine the results from Wikidata and DBpedia\n",
        "    disambiguated_entities = {\n",
        "        \"wikidata\": disambiguated_wikidata_entities,\n",
        "        \"dbpedia\": disambiguated_dbpedia_entities\n",
        "    }\n",
        "\n",
        "    return disambiguated_entities"
      ],
      "metadata": {
        "id": "CUkHtwaqTDTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in docs:\n",
        "  doc.wiki_entities = {}\n",
        "  entities = {}\n",
        "  for j in range(0, len(doc.raw_text), 4000):\n",
        "    doc.wiki_entities |= disambiguate_with_llm(doc.raw_text[j:j+4000])"
      ],
      "metadata": {
        "id": "3v2QQmase3CD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the entities disambiguated according to DBpedia and Wikidata"
      ],
      "metadata": {
        "id": "5cC1zoDSe5BF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìù TODO: Display extracted Wikidata entities for the first document\n",
        "if docs:\n",
        "    first_doc = docs[0]\n",
        "    print(\"Extracted Wikidata entities for the first document:\")\n",
        "    print(first_doc.wiki_entities)"
      ],
      "metadata": {
        "id": "B80_ZKM7SoIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the documents and populate the dbpedia_entities\n",
        "for doc in docs:\n",
        "    doc.dbpedia_entities = {}  # Initialize the dbpedia_entities attribute\n",
        "    entities = {}\n",
        "    for j in range(0, len(doc.raw_text), 4000):\n",
        "        doc.dbpedia_entities |= disambiguate_with_llm(doc.raw_text[j:j+4000])\n",
        "\n",
        "# Display extracted DBpedia entities for the first document\n",
        "if docs:\n",
        "    first_doc = docs[0]\n",
        "    print(\"Extracted DBpedia entities for the first document:\")\n",
        "    print(first_doc.dbpedia_entities)"
      ],
      "metadata": {
        "id": "VJs0r4buN9tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Run relation extraction"
      ],
      "metadata": {
        "id": "fNvv056SGirj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6a: Using OpenIE\n",
        "\n",
        "We will use [Stanford OpenIE](https://nlp.stanford.edu/software/openie.html) to extract the relations between the entities in the input text."
      ],
      "metadata": {
        "id": "nDLkGwsxrsOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pycorenlp import StanfordCoreNLP\n",
        "\n",
        "# Create a StanfordCoreNLP object\n",
        "nlp = StanfordCoreNLP('https://websem:eurecom@corenlp.tools.eurecom.fr')\n",
        "\n",
        "# Define a function to extract relations from input text using Stanford OpenIE\n",
        "def extract_relations_with_openie(input_text, language):\n",
        "  output = nlp.annotate(input_text, properties={\n",
        "    'timeout': 300000,\n",
        "    'annotators': 'tokenize,ssplit,openie',\n",
        "    'outputFormat': 'json',\n",
        "    'pipelineLanguage': language[:2]\n",
        "  })\n",
        "  try:\n",
        "    output = json.loads(output)\n",
        "  except Exception as err:\n",
        "    print(f'Unexpected response: {output}')\n",
        "    raise\n",
        "\n",
        "  # üìù TODO: Get relations from the `output` object (subject, relation, object)\n",
        "  #    and append them to a `extracted_relations` list.\n",
        "  # üí° You can start by printing the `output` object to understand its structure.\n",
        "  # Initialize the list to store extracted relations\n",
        "  extracted_relations = []\n",
        "\n",
        "  for sentence in output.get('sentences', []):\n",
        "      for triple in sentence.get('openie', []):\n",
        "          subject = triple.get('subject')\n",
        "          relation = triple.get('relation')\n",
        "          object_ = triple.get('object')\n",
        "\n",
        "          if subject and relation and object_:\n",
        "              extracted_relations.append({\n",
        "                  'subject': subject,\n",
        "                  'relation': relation,\n",
        "                  'object': object_\n",
        "              })\n",
        "\n",
        "  # Return relations\n",
        "  return extracted_relations"
      ],
      "metadata": {
        "id": "mVVKYu3CGjaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in docs:\n",
        "  if doc.language == \"english\":  # CoreNLP OpenIE only supports english\n",
        "    doc.relations = extract_relations_with_openie(doc.raw_text, doc.language)"
      ],
      "metadata": {
        "id": "Wa4T96f1f2Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display relations which have been extracted:"
      ],
      "metadata": {
        "id": "zSSohZ89_7Xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìù TODO: Display extracted relations for the first document\n",
        "if docs:\n",
        "    first_doc = docs[0]\n",
        "    print(\"Extracted relations for the first document:\")\n",
        "    print(first_doc.relations)"
      ],
      "metadata": {
        "id": "VI5oEP91-l66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6b: Using LLMs\n",
        "\n",
        "As an alternative to OpenIE, we will use LLMs to do the same task and compare the results."
      ],
      "metadata": {
        "id": "J1pKG1UDWc76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_relations_with_llm(text):\n",
        "    # üìù TODO: Create a prompt and query the LLM to get relations (subject, relation, object)\n",
        "    #    from the text and append them to a `extracted_relations` list.\n",
        "    payload = {\n",
        "        \"prompt\": f\"\"\"\n",
        "        Extract relations (subject, relation, object) from the following text:\n",
        "\n",
        "        Text:\n",
        "        {text}\n",
        "\n",
        "        Provide the result in the following JSON structure:\n",
        "        [\n",
        "            {{ \"subject\": \"<subject_text>\", \"relation\": \"<relation_text>\", \"object\": \"<object_text>\" }},\n",
        "            ...\n",
        "        ]\n",
        "        \"\"\",\n",
        "        \"format\": {\n",
        "            \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"subject\": {\"type\": \"string\"},\n",
        "                    \"relation\": {\"type\": \"string\"},\n",
        "                    \"object\": {\"type\": \"string\"}\n",
        "                },\n",
        "                \"required\": [\"subject\", \"relation\", \"object\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Call the LLM generate function\n",
        "    llm_response = llm_generate(payload)\n",
        "\n",
        "    # Check if the response is valid\n",
        "    if llm_response and \"response\" in llm_response:\n",
        "        try:\n",
        "            # Parse the JSON response\n",
        "            extracted_relations = json.loads(llm_response[\"response\"])\n",
        "            return extracted_relations\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Error decoding JSON response.\")\n",
        "            return []\n",
        "    else:\n",
        "        print(\"Error generating response from LLM.\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "pF0n8sChWieD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in docs:\n",
        "  doc.llm_relations = extract_relations_with_llm(doc.raw_text)"
      ],
      "metadata": {
        "id": "3J4QePdoWoiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display relations which have been extracted:"
      ],
      "metadata": {
        "id": "soquKeV6WoiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the extracted relations for the first document\n",
        "display(docs[0].llm_relations)"
      ],
      "metadata": {
        "id": "7k1TEbohWoiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Implement some mappings between the entity types and relations returned with a given cycling ontology\n",
        "We will implement mappings between the entity types and relations returned with the cycling ontology available at https://nextcloud.eurecom.fr/s/yKaMDEnRoSqjNAL."
      ],
      "metadata": {
        "id": "A_YdNyfHGlC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rdflib\n",
        "from rdflib import Graph, URIRef, Literal, Namespace\n",
        "\n",
        "g = Graph()\n",
        "\n",
        "CYCLING = Namespace(\"http://example.org/cycling#\")\n",
        "WIKI = Namespace(\"http://example.org/wiki#\")\n",
        "\n",
        "g.bind(\"cycling\", CYCLING)\n",
        "g.bind(\"wiki\", WIKI)\n",
        "\n",
        "entities_en = [\n",
        "    {\"id\": \"cyclist1\", \"name\": \"Chris Froome\", \"type\": \"Cyclist\"},\n",
        "    {\"id\": \"team1\", \"name\": \"Team Sky\", \"type\": \"Team\"},\n",
        "]\n",
        "\n",
        "relations_en = [\n",
        "    {\"subject\": \"cyclist1\", \"predicate\": \"is_part_of\", \"object\": \"team1\"},\n",
        "]\n",
        "\n",
        "wiki_entities_en = [\n",
        "    {\"entity\": \"Chris Froome\", \"wiki_url\": \"https://en.wikipedia.org/wiki/Chris_Froome\"},\n",
        "]\n",
        "\n",
        "for entity in entities_en:\n",
        "    entity_uri = URIRef(CYCLING[entity[\"id\"]])\n",
        "    g.add((entity_uri, rdflib.RDF.type, URIRef(CYCLING[entity[\"type\"]])))\n",
        "    g.add((entity_uri, CYCLING.name, Literal(entity[\"name\"])))\n",
        "\n",
        "    for wiki in wiki_entities_en:\n",
        "        if wiki[\"entity\"] == entity[\"name\"]:\n",
        "            g.add((entity_uri, CYCLING.hasWikiLink, URIRef(wiki[\"wiki_url\"])))\n",
        "\n",
        "for relation in relations_en:\n",
        "    subject_uri = URIRef(CYCLING[relation[\"subject\"]])\n",
        "    predicate_uri = URIRef(CYCLING[relation[\"predicate\"]])\n",
        "    object_uri = URIRef(CYCLING[relation[\"object\"]])\n",
        "\n",
        "    g.add((subject_uri, predicate_uri, object_uri))\n",
        "\n",
        "print(g.serialize(format=\"turtle\"))"
      ],
      "metadata": {
        "id": "uwo01wgOGmMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the result into a file\n",
        "g.serialize(destination='output.ttl')"
      ],
      "metadata": {
        "id": "Nq4jHBEsUxCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Load the data in the Corese engine with the ontology and write the SPARQL queries to retrieve specific information from the KG\n",
        "We will load the data in the [Corese](http://wimmics.inria.fr/doc/tutorial/corese-3.2.3c.jar) engine (the same you used in the Assignment 2) with the ontology and write the SPARQL queries to retrieve specific information from the KG. We will write the following queries:"
      ],
      "metadata": {
        "id": "VJXcKJUZGmqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* üìù List the name of the cycling teams"
      ],
      "metadata": {
        "id": "cSLn1iZB3BUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PREFIX : <http://example.org/ontology#>\n",
        "\n",
        "SELECT ?team_name\n",
        "WHERE {\n",
        "  ?team a :CyclingTeam ;\n",
        "        :hasName ?team_name .\n",
        "}"
      ],
      "metadata": {
        "id": "zPpul2UUC5QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* üìù List the name of the cycling riders"
      ],
      "metadata": {
        "id": "K0ITJ6LO3C9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PREFIX : <http://example.org/ontology#>\n",
        "\n",
        "SELECT ?rider_name\n",
        "WHERE {\n",
        "  ?rider a :CyclingRider ;\n",
        "         :hasName ?rider_name .\n",
        "}"
      ],
      "metadata": {
        "id": "C5BUGZ84C-B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* üìù Retrieve the name of the winner of the Prologue"
      ],
      "metadata": {
        "id": "t5u5FyOq3FjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PREFIX : <http://example.org/ontology#>\n",
        "\n",
        "SELECT ?winner_name\n",
        "WHERE {\n",
        "  ?race a :Prologue ;\n",
        "        :hasWinner ?winner .\n",
        "  ?winner :hasName ?winner_name .\n",
        "}"
      ],
      "metadata": {
        "id": "FT-Zos-MDAaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù We will also write the same 3 queries on Wikidata starting from `Q98043180` to compare the results."
      ],
      "metadata": {
        "id": "kRpBPPYR3HTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SELECT ?team_name WHERE {\n",
        "  ?team wdt:P31 wd:Q13406325;  # Cycling team (Q13406325)\n",
        "        rdfs:label ?team_name.\n",
        "  FILTER(LANG(?team_name) = \"en\")\n",
        "}"
      ],
      "metadata": {
        "id": "TDjG2cJzDH4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SELECT ?rider_name WHERE {\n",
        "  ?rider wdt:P31 wd:Q13393280;  # Cyclist (Q13393280)\n",
        "         rdfs:label ?rider_name.\n",
        "  FILTER(LANG(?rider_name) = \"en\")\n",
        "}"
      ],
      "metadata": {
        "id": "LRNFskPVDIzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SELECT ?winner_name WHERE {\n",
        "  ?race wdt:P31 wd:Q31431;  # Prologue (Q31431)\n",
        "        wdt:P1344 ?winner.  # Winner (P1344)\n",
        "  ?winner rdfs:label ?winner_name.\n",
        "  FILTER(LANG(?winner_name) = \"en\")\n",
        "}"
      ],
      "metadata": {
        "id": "5jfy6xUoDKk_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}